{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-31T17:38:27.908985400Z",
     "start_time": "2023-05-31T17:38:21.245320400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\modze\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import TweetTokenizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "import certifi\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import openpyxl\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stałe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "ACRONYMS = {\n",
    "    \"cuz\": \"because\",\n",
    "    \"tn\": \"tonight\",\n",
    "    \"dr\": \"doctor\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"ny\": \"new york\",\n",
    "    \"afaik\": \"as far as i know\",\n",
    "    \"bts\": \"behind the scenes\",\n",
    "    \"cba\": \"can't be bothered\",\n",
    "    \"dnd\": \"do not disturb\",\n",
    "    \"eli5\": \"explain like i'm 5\",\n",
    "    \"fomc\": \"federal open market committee\",\n",
    "    \"g2g\": \"got to go\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"kpi\": \"key performance indicator\",\n",
    "    \"nbd\": \"no big deal\",\n",
    "    \"potus\": \"president of the united states\",\n",
    "    \"rip\": \"rest in peace\",\n",
    "    \"tl;dr\": \"too long; didn't read\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"ofc\": \"of course\",\n",
    "    \"otw\": \"on the way\",\n",
    "    \"pls\": \"please\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"wth\": \"what the heck\",\n",
    "    \"wysiwyg\": \"what you see is what you get\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"rt\": \"retweet\",\n",
    "    \"ct\": \"cuttweet\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"tgif\": \"thank god it's friday\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hbd\": \"happy birthday\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ootd\": \"outfit of the day\",\n",
    "    \"roflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"tbt\": \"throwback thursday\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"ama\": \"ask me anything\",\n",
    "    \"fwiw\": \"for what it's worth\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"ootw\": \"outfit of the week\",\n",
    "    \"pos\": \"piece of shit\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"wcw\": \"woman crush wednesday\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"nm\": \"not much\",\n",
    "    \"nsfw\": \"not safe for work\",\n",
    "    \"ootn\": \"outfit of the night\",\n",
    "    \"smdh\": \"shaking my damn head\",\n",
    "    \"til\": \"today i learned\",\n",
    "    \"wdyt\": \"what do you think\",\n",
    "    \"prt\": \"partial retweet\",\n",
    "    \"mt\": \"modified tweet\",\n",
    "    \"cx\": \"correction\",\n",
    "    \"seo\": \"search engine optimization\",\n",
    "    \"sroi\": \"social return on investment\",\n",
    "    \"sn\": \"social network\",\n",
    "    \"yt\": \"youtube\",\n",
    "    \"ugc\": \"user-generated content\",\n",
    "    \"smo\": \"social media optimization\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"sm\": \"social media\",\n",
    "    \"smm\": \"social media marketing\",\n",
    "    \"ezine\": \"electronic magazine\",\n",
    "    \"bgd\": \"background\",\n",
    "    \"abt\": \"about\",\n",
    "    \"ab\": \"about\",\n",
    "    \"dd\": \"dear daughter\",\n",
    "    \"ayfkmwts\": \"are you f—ing kidding me with this s—?\",\n",
    "    \"br\": \"best regards\",\n",
    "    \"chk\": \"check\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"dp\": \"display picture\",\n",
    "    \"fml\": \"fuck my life\",\n",
    "    \"fubar\": \"fucked up beyond all repair\",\n",
    "    \"bbfn\": \"bye for now\",\n",
    "    \"b4\": \"before\",\n",
    "    \"ds\": \"dear son\",\n",
    "    \"ema\": \"email address\",\n",
    "    \"dyk\": \"do you know\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"ftf\": \"face to face\",\n",
    "    \"hagn\": \"have a good night\",\n",
    "    \"df\": \"dear fiancé\",\n",
    "    \"dam\": \"don't annoy me\",\n",
    "    \"ffs\": \"for fuck sake\",\n",
    "    \"em\": \"email\",\n",
    "    \"eml\": \"email\",\n",
    "    \"fotd\": \"find of the day\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"hth\": \"hope that helps\",\n",
    "    \"gmafb\": \"give me a f—ing break\",\n",
    "    \"hand\": \"have a nice day\",\n",
    "    \"gtfooh\": \"get the fuck out of here\",\n",
    "    \"gts\": \"guess the song\",\n",
    "    \"hotd\": \"headline of the day\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"kyso\": \"knock your socks off\",\n",
    "    \"kk\": \"ok\",\n",
    "    \"ht\": \"head through\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"lhh\": \"hella hard (a stronger version of lol)\",\n",
    "    \"zomg\": \"omg to the max\",\n",
    "    \"nfw\": \"no f—ing way\",\n",
    "    \"orly\": \"oh, really?\",\n",
    "    \"yoyo\": \"you're on your own\",\n",
    "    \"iwsn\": \"i want sex now\",\n",
    "    \"jv\": \"joint venture\",\n",
    "    \"lo\": \"little one\",\n",
    "    \"jsyk\": \"just so you know\",\n",
    "    \"nsw\": \"not safe work\",\n",
    "    \"mrjn\": \"marijuana\",\n",
    "    \"mirl\": \"meet in real life\",\n",
    "    \"nct\": \"nobody cares, though\",\n",
    "    \"njoy\": \"enjoy\",\n",
    "    \"omfg\": \"oh my f—ing god\",\n",
    "    \"pnp\": \"party and play (drugs and sex)\",\n",
    "    \"qotd\": \"quote of the day\",\n",
    "    \"sfw\": \"safe for work\",\n",
    "    \"oomf\": \"one of my friends/followers\",\n",
    "    \"nts\": \"note to self\",\n",
    "    \"rtfm\": \"read the f—ing manual\",\n",
    "    \"snafu\": \"situation normal, all f—ed up\",\n",
    "    \"rlrt\": \"real-life re-tweet, a close cousin to oh\",\n",
    "    \"stfw\": \"search the f—ing web!\",\n",
    "    \"tftt\": \"thanks for this tweet\",\n",
    "    \"sob\": \"son of a b—-\",\n",
    "    \"tftf\": \"thanks for the follow\",\n",
    "    \"rtq\": \"significant other\",\n",
    "    \"tj\": \"tweetjack, or joining a conversation belatedly to contribute to a tangent\",\n",
    "    \"srs\": \"serious\",\n",
    "    \"stf\": \"shut the f—\",\n",
    "    \"stfu\": \"shut the f— up!\",\n",
    "    \"tl\": \"timeline\",\n",
    "    \"tyia\": \"thank you in advance\",\n",
    "    \"tt\": \"trending topic\",\n",
    "    \"tldr\": \"too long didn’t read\",\n",
    "    \"tmb\": \"tweet me back\",\n",
    "    \"tyvw\": \"thank you very much\",\n",
    "    \"wtv\": \"whatever\",\n",
    "    \"ymmv\": \"your mileage may vary\",\n",
    "    \"ykyat\": \"you know you’re addicted to\",\n",
    "    \"w/e\": \"whatever\",\n",
    "    \"tyt\": \"take your time\",\n",
    "    \"ykwim\": \"you know what i mean\",\n",
    "    \"ygtr\": \"you got that right\",\n",
    "    \"li\": \"linkedin\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"re\": \"reply\",\n",
    "    \"s/o\": \"shout out\",\n",
    "    \"ig\": \"instagram\",\n",
    "    \"cmv\": \"change my view\",\n",
    "    \"fbf\": \"flashback friday\",\n",
    "    \"oc\": \"original content\",\n",
    "    \"wbw\": \"way back wednesday\",\n",
    "    \"eta\": \"estimated time of arrival\",\n",
    "    \"smart\": \"specific, measurable, attainable, relevant, timely\",\n",
    "    \"smp\": \"social media platform\"\n",
    "}\n",
    "STOPWORDS = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\",\n",
    "             \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\",\n",
    "             \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\",\n",
    "             \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\n",
    "             \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\",\n",
    "             \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\",\n",
    "             \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\",\n",
    "             \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\",\n",
    "             \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\",\n",
    "             \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\",\n",
    "             \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\",\n",
    "             \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\",\n",
    "             \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\",\n",
    "             \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\",\n",
    "             \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\",\n",
    "             \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\",\n",
    "             \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\",\n",
    "             \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\",\n",
    "             \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\",\n",
    "             \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\",\"doesnt\", \"doing\", \"don\",\"dont\", \"done\", \"don't\", \"down\",\n",
    "             \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\",\n",
    "             \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\",\n",
    "             \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\",\n",
    "             \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\",\n",
    "             \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\",\n",
    "             \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\",\n",
    "             \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\",\n",
    "             \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\",\n",
    "             \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\",\n",
    "             \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\",\n",
    "             \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\",\n",
    "             \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\",\n",
    "             \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\",\n",
    "             \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\",\n",
    "             \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\",\n",
    "             \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\",\n",
    "             \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\",\n",
    "             \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\",\n",
    "             \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\",\n",
    "             \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\",\n",
    "             \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\",\n",
    "             \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\",\n",
    "             \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\",\n",
    "             \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\",\n",
    "             \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\",\n",
    "             \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\",\n",
    "             \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\",\n",
    "             \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\",\n",
    "             \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\",\n",
    "             \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\",\n",
    "             \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\",\n",
    "             \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\",\n",
    "             \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\",\n",
    "             \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\",\n",
    "             \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\",\n",
    "             \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\",\n",
    "             \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\",\n",
    "             \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\",\n",
    "             \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\",\n",
    "             \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\",\n",
    "             \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\",\n",
    "             \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\",\n",
    "             \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\",\n",
    "             \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\",\n",
    "             \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\",\n",
    "             \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\",\n",
    "             \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\",\n",
    "             \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\",\n",
    "             \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\",\n",
    "             \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\",\n",
    "             \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\",\n",
    "             \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\",\n",
    "             \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\",\n",
    "             \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\",\n",
    "             \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "             \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\",\n",
    "             \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\",\n",
    "             \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\",\n",
    "             \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\",\n",
    "             \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\",\n",
    "             \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\",\n",
    "             \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\",\n",
    "             \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\",\n",
    "             \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\",\n",
    "             \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\",\n",
    "             \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\",\n",
    "             \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\",\n",
    "             \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\",\n",
    "             \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\",\n",
    "             \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\",\n",
    "             \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\",\n",
    "             \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\",\n",
    "             \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\",\n",
    "             \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\",\n",
    "             \"zero\", \"zi\", \"zz\"]\n",
    "CLEANING_REGEX = \"[@#]\\w+|https?:\\S+|<\\w+>|\\S+@\\w+.\\w+|[^A-Za-z]\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T17:38:27.963019600Z",
     "start_time": "2023-05-31T17:38:27.904985100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ładowanie Tweet'ów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Twitter data.csv\", index_col='Unnamed: 0')\n",
    "df.drop(columns=['_id', 'created_at', 'id', 'name', 'user_created', 'verified', 'preprocessed_text', 'token_count',\n",
    "                 'untokenized_text'], inplace=True)\n",
    "df['mentions'] = df['text'].apply(lambda x: re.findall(\"@([a-zA-Z0-9_]{1,50})\", str(x)))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wyekstrachowanie 2 pierwszych osób"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_mentions(list, index):\n",
    "    try:\n",
    "        return list[index]\n",
    "    except:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T17:27:36.478127400Z",
     "start_time": "2023-05-31T17:27:36.472125500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df['first_mention'] = df['mentions'].apply(lambda x: get_mentions(x, 0))\n",
    "df['second_mention'] = df['mentions'].apply(lambda x: get_mentions(x, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T17:28:35.968962300Z",
     "start_time": "2023-05-31T17:28:35.893161900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Oczyszczanie tekstu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0             [lovely, boy, reply, cute, feel, warmth, god]\n1         [concordia, prof, globe, gazette, york, daily,...\n2                  [kim, mulke, fly, hell, face, tear, joy]\n3                   [caitlin, clark, multitude, rebounders]\n4                                   [nova, day, highschool]\n                                ...                        \n142653    [nice, day, watched, minute, botw, coverage, c...\n142654                                        [shirt, hair]\n142655    [hype, honest, botw, shot, anticipation, insta...\n142656    [based, simple, logic, tear, kingdom, good, bo...\n142657                                                [vpn]\nName: preprocessed_text, Length: 87581, dtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess(text, emojis=True):\n",
    "    if emojis:\n",
    "        text = emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "    text = re.sub(CLEANING_REGEX, ' ', str(text).lower()).strip()\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in ACRONYMS:\n",
    "            expanded_tokens = ACRONYMS[tokens[i]].split()\n",
    "            tokens[i:i + 1] = expanded_tokens\n",
    "\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS if token if len(token) > 1]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "df[\"preprocessed_text\"] = df[\"text\"].apply(lambda text: preprocess(text))\n",
    "df[\"preprocessed_text\"].dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T17:39:42.726600200Z",
     "start_time": "2023-05-31T17:38:36.908040100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        preprocessed_text first_mention  \\\n0           [lovely, boy, reply, cute, feel, warmth, god]      artcmiis   \n1       [concordia, prof, globe, gazette, york, daily,...        Snellk   \n2                [kim, mulke, fly, hell, face, tear, joy]          None   \n3                 [caitlin, clark, multitude, rebounders]          None   \n4                                 [nova, day, highschool]          None   \n...                                                   ...           ...   \n142653  [nice, day, watched, minute, botw, coverage, c...      jj_mason   \n142654                                      [shirt, hair]     C_Dobbins   \n142655  [hype, honest, botw, shot, anticipation, insta...          None   \n142656  [based, simple, logic, tear, kingdom, good, bo...          None   \n142657                                              [vpn]        _aalsy   \n\n       second_mention  \n0       Dream__Fanart  \n1                None  \n2                None  \n3                None  \n4                None  \n...               ...  \n142653           None  \n142654           None  \n142655           None  \n142656           None  \n142657           None  \n\n[87581 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>first_mention</th>\n      <th>second_mention</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[lovely, boy, reply, cute, feel, warmth, god]</td>\n      <td>artcmiis</td>\n      <td>Dream__Fanart</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[concordia, prof, globe, gazette, york, daily,...</td>\n      <td>Snellk</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[kim, mulke, fly, hell, face, tear, joy]</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[caitlin, clark, multitude, rebounders]</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[nova, day, highschool]</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>142653</th>\n      <td>[nice, day, watched, minute, botw, coverage, c...</td>\n      <td>jj_mason</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>142654</th>\n      <td>[shirt, hair]</td>\n      <td>C_Dobbins</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>142655</th>\n      <td>[hype, honest, botw, shot, anticipation, insta...</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>142656</th>\n      <td>[based, simple, logic, tear, kingdom, good, bo...</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>142657</th>\n      <td>[vpn]</td>\n      <td>_aalsy</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>87581 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"preprocessed_text\",\"first_mention\",\"second_mention\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T17:40:19.773664Z",
     "start_time": "2023-05-31T17:40:19.739128700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
